# Part 2: Processing the data

## Setup

Make sure you have `lcpcli` installed in your local python environment:

```bash
pip install lcpcli==0.2.2
```

The `lcpcli` library provides us with a tool to help us build LCP corpora in python.

## Corpus builder

At the simplest level, creating an LCP corpus requires very few steps:

```python
from lcpcli.builder import Corpus

corpus = Corpus("my corpus")
corpus.Document(
    corpus.Segment(
        corpus.Token("Hello"),
        corpus.Token("world")
    )
)
corpus.make()
```

The lines above will create a text-only corpus consisting of only one document, with only one segment, with two tokens whose forms are `Hello` and `world`.

The last line, `c.make()`, builds and outputs files for upload in the current directory.

Our application will be a little more sophisticated that this hello-world example (we will have multiple segments, and eventually two documents) but the basic idea will remain the same.


## Segmentation and Tokenization

The `Corpus` class allows us to define arbitrary names instead of `Token`, `Segment` and `Document`. In this tutorial, we will use the more common (albeit less technical) terms `Word`, `Sentence` and `Clip` instead:

```python
corpus = Corpus(
    "Tutorial corpus",
    document="Clip",
    segment="Sentence",
    token="Word"
)
```

To start with, we will only proceed a single SRT file, `database_explorer.srt`, so we will only create a single clip in the corpus, using `clip = corpus.Clip()`

A commented python script that generates a _non-time-aligned corpus_ using the `Corpus` class can be found [here](https://github.com/liri-uzh/lcp_tutorial/blob/main/first_pass/convert.py).

The script reads one line from an SRT file at a time:

 - If the line reports a block number or the block's timestamps, it is ignored
 - If the line is a linebreak, the next two lines are flagged as a block number and its timestamps
 - If the line has some text, it is split by sentence-delimiters (`.`, `!`, `?`) and each resulting chunk is in turn split by token-delimiters (` `, `,`, `'`); the resulting word is added to the current sentence

Whenever the script hits a sentence delimiter, it creates a new sentence with `sentence = clip.Sentence()`, and whenever it processes a word, it adds it to the current sentence with `sentence.Word(w)`.

{% hint style="info" %}
Note that `.Sentence` can be called either on `clip`, as in this script, or on `corpus`, which is the method used in the hello-world example (with `.Segment`). In the hello-world, the segment is created directly as an argument of `corpus.Document` and, as such, belongs to it; here, the script does not pass sentences as direct arguments of the clip; instead, the sentences are created on the fly, which why it is necessary to call `.Sentence` on `clip`.

The same logic applies to calling `.Word` on `corpus` vs on `sentence`. 
{% endhint %}

### Import

Visit [catchphrase](https://catchphrase.linguistik.uzh.ch) and create a new corpus collection, then open its setting by clicking the gear icon, click the "API" tab and create a new API key; write down the key and the secret.

Open a terminal and run the following command, replacing `path/to/output` with a path pointing to the files generated by the script, `$API_KEY` with the key you just got, `$API_SECRET` with the secret you just got and `$PROJECT` with the name of the collection you just created:

`lcpcli -c path/to/output/ -k $API_KEY -s $API_SECRET -p "$PROJECT" --live`

You should get a confirmation message, and your corpus should now be visible in your collection after you refresh the page!


### Multiple documents

To parse multilpe SRT files, the edits to the script are minimal: one just needs to iterate over the SRT files to create a dedicated instance of `Clip` for each, and one can also report the name of the file for reference purposes, as in: `clip = corpus.Clip(name="database_explorer")`.

{% hint style="info" %}
One can pass arbitrary keyword arguments to define attributes, as illustrated here with `name="database_explorer"`. Another option is to set the attribute on the instance, as in `clip.name = "database_explorer"`. This applies to all entities, not just to documents.
{% endhint %}

An updated version of the script that processes all the SRT documents can be found [here](https://github.com/liri-uzh/lcp_tutorial/blob/main/all_documents/convert.py).

### Adding annotations

For the sake of illustration, we will now add two pieces of annotation: we will report the original text of each sentence (which will include the token-delimiter characters) and we will report whether each word was preceded by a single quote.

The original text of the sentence is set as `sentence.original = original` (where `original` is a string containing the original text) and whether a word is preceded by a `'` is set when creating the word instance, as in `sentence.Word(word, shortened=shortened` (where `shortened` is appropriately set to `"yes"` or `"no"`).

An updated python script can be found [here](https://github.com/liri-uzh/lcp_tutorial/blob/main/original_shortened/convert.py).


### Time alignment and video

An updated python script can be found [here](https://github.com/liri-uzh/lcp_tutorial/blob/main/video/convert.py). Make sure to place the files database_explorer.mp4 and presenter_pro.mp4 in your folder so they can be included in the output folder and uploaded to _videoScope_.